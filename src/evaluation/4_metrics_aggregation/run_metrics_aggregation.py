"""
è¯„ä¼°æŒ‡æ ‡æ±‡æ€»è¿è¡Œè„šæœ¬
æ”¶é›†æ‰€æœ‰è¯„ä¼°æ­¥éª¤çš„ç»“æœï¼Œè®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡
"""

import argparse
import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from src.evaluation.config_new.evaluation_config import (
    RESULTS_CONFIG, get_results_path, ensure_results_directories
)
from metrics_aggregator import MetricsAggregator


def main():
    parser = argparse.ArgumentParser(description="æ±‡æ€»è¯„ä¼°æŒ‡æ ‡ï¼Œç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")

    parser.add_argument(
        "--model_type",
        type=str,
        choices=["base_model", "lora_model"],
        default="base_model",
        help="æ¨¡å‹ç±»å‹é€‰æ‹© (é»˜è®¤: all - å¤„ç†æ‰€æœ‰æ¨¡å‹ç±»å‹)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„)"
    )
    parser.add_argument(
        "--generate_report",
        action="store_true",
        help="ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"
    )

    args = parser.parse_args()

    print("=" * 60)
    print("è¯„ä¼°æŒ‡æ ‡æ±‡æ€»è„šæœ¬")
    print("=" * 60)
    print(f"æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š: {'æ˜¯' if args.generate_report else 'å¦'}")
    print("=" * 60)

    try:
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        ensure_results_directories()

        # åˆå§‹åŒ–æŒ‡æ ‡æ±‡æ€»å™¨
        aggregator = MetricsAggregator()

        # ç¡®å®šè¦å¤„ç†çš„æ¨¡å‹ç±»å‹
        if args.model_type == "all":
            model_types = ["base_model", "lora_model"]
        else:
            model_types = [args.model_type]

        # è®¾ç½®è¾“å‡ºç›®å½•
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            output_dir = RESULTS_CONFIG["final_metrics"]["base_dir"]

        output_dir.mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ¯ç§æ¨¡å‹ç±»å‹
        all_model_metrics = {}

        for model_type in model_types:
            print(f"\næ­£åœ¨å¤„ç† {model_type}...")

            # åŠ è½½æ‰€æœ‰è¯„ä¼°ç»“æœ
            print("  1. åŠ è½½è¯„ä¼°ç»“æœ...")
            all_results = aggregator.load_all_results(model_type)

            if not any(all_results.values()):
                print(f"  è­¦å‘Š: {model_type} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¯„ä¼°ç»“æœï¼Œè·³è¿‡")
                continue

            # åˆå¹¶ç»“æœ
            print("  2. åˆå¹¶è¯„ä¼°ç»“æœ...")
            merged_results = aggregator.merge_results(all_results, model_type)

            # è®¡ç®—æŒ‡æ ‡
            print("  3. è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
            metrics = aggregator.calculate_metrics(merged_results)
            metrics["model_type"] = model_type
            all_model_metrics[model_type] = metrics

            # ä¿å­˜è¯¦ç»†ç»“æœ
            if args.generate_report:
                detailed_path = output_dir / f"{model_type}_detailed_results.csv"
                print(f"  4. ä¿å­˜è¯¦ç»†ç»“æœåˆ° {detailed_path}...")
                aggregator.save_detailed_results(merged_results, detailed_path)

        # ä¿å­˜æœ€ç»ˆæŒ‡æ ‡
        print("\nä¿å­˜æœ€ç»ˆæŒ‡æ ‡...")
        metrics_path = output_dir / RESULTS_CONFIG["final_metrics"]["metrics_json"]

        if len(model_types) == 1:
            # å•ä¸ªæ¨¡å‹ç±»å‹
            final_metrics = list(all_model_metrics.values())[0]
        else:
            # å¤šä¸ªæ¨¡å‹ç±»å‹å¯¹æ¯”
            final_metrics = {
                "evaluation_comparison": all_model_metrics,
                "evaluation_timestamp": all_model_metrics[model_types[0]]["evaluation_timestamp"],
                "total_models_evaluated": len(all_model_metrics)
            }

        aggregator.save_metrics(final_metrics, metrics_path)

        # ç”Ÿæˆå¹¶æ‰“å°æ‘˜è¦æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("è¯„ä¼°å®Œæˆ - æ‘˜è¦æŠ¥å‘Š")
        print("=" * 60)

        for model_type, metrics in all_model_metrics.items():
            print(f"\nğŸ“Š {model_type.upper()} æ¨¡å‹è¯„ä¼°ç»“æœ:")
            print(f"  - æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
            print(f"  - SQLå¯æ‰§è¡Œç‡: {metrics['execution_validity']['validity_rate']:.2%}")
            print(f"  - è¯­ä¹‰ç­‰ä»·ç‡: {metrics['semantic_equivalence']['equivalence_rate']:.2%}")

        if len(model_types) > 1:
            print("\nğŸ”„ æ¨¡å‹å¯¹æ¯”:")
            base_acc = all_model_metrics["base_model"]["overall_correctness"]["correctness_rate"]
            lora_acc = all_model_metrics["lora_model"]["overall_correctness"]["correctness_rate"]
            improvement = lora_acc - base_acc
            print(f"  - LoRA vs Base å‡†ç¡®ç‡æå‡: {improvement:+.2%}")
            print(f"  - Baseæ¨¡å‹å‡†ç¡®ç‡: {base_acc:.2%}")
            print(f"  - LoRAæ¨¡å‹å‡†ç¡®ç‡: {lora_acc:.2%}")

        print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®:")
        print(f"  - æŒ‡æ ‡æ–‡ä»¶: {metrics_path}")
        if args.generate_report:
            summary_path = output_dir / f"evaluation_summary.txt"
            with open(summary_path, 'w', encoding='utf-8') as f:
                for model_type, metrics in all_model_metrics.items():
                    report = aggregator.generate_summary_report(metrics)
                    f.write(f"{model_type.upper()} æ¨¡å‹\n")
                    f.write(report + "\n\n")
            print(f"  - æ‘˜è¦æŠ¥å‘Š: {summary_path}")

        print("=" * 60)
        print("è¯„ä¼°æŒ‡æ ‡æ±‡æ€»å®Œæˆï¼")
        return 0

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())