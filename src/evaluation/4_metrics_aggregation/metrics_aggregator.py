"""
è¯„ä¼°æŒ‡æ ‡æ±‡æ€»å™¨
æ”¶é›†æ‰€æœ‰è¯„ä¼°æ­¥éª¤çš„ç»“æœï¼Œè®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡
æ”¯æŒæŒ‰ç…§"è¯­ä¹‰ç­‰ä»· OR æŸ¥è¯¢åŒ¹é…"è§„åˆ™åˆ¤æ–­æœ€ç»ˆæ­£ç¡®æ€§
"""

import json
import logging
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# Add parent directory to path
import sys
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from src.evaluation.config_new.evaluation_config import (
    RESULTS_CONFIG, FINAL_EVALUATION_RULES, EVALUATION_CRITERIA
)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MetricsAggregator:
    """è¯„ä¼°æŒ‡æ ‡æ±‡æ€»å™¨ï¼Œç”¨äºè®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡"""

    def __init__(self):
        """åˆå§‹åŒ–æŒ‡æ ‡æ±‡æ€»å™¨"""
        self.evaluation_rule = FINAL_EVALUATION_RULES["success_condition"]
        self.rule_description = FINAL_EVALUATION_RULES["description"]

    def load_all_results(self, model_type: str = "base_model") -> Dict[str, List[Dict]]:
        """
        åŠ è½½æ‰€æœ‰è¯„ä¼°æ­¥éª¤çš„ç»“æœ

        Args:
            model_type: æ¨¡å‹ç±»å‹ ("base_model" æˆ– "lora_model")

        Returns:
            åŒ…å«æ‰€æœ‰æ­¥éª¤ç»“æœçš„å­—å…¸
        """
        results = {}

        # 1. åŠ è½½SQLç”Ÿæˆç»“æœ
        try:
            gen_path = RESULTS_CONFIG["sql_generation"]["base_dir"] / RESULTS_CONFIG["sql_generation"][model_type]
            results["sql_generation"] = self._load_results_file(gen_path)
            logger.info(f"Loaded {len(results['sql_generation'])} SQL generation results")
        except FileNotFoundError:
            logger.warning(f"SQL generation results not found for {model_type}")
            results["sql_generation"] = []

        # 2. åŠ è½½æ‰§è¡ŒéªŒè¯ç»“æœ
        try:
            exec_path = RESULTS_CONFIG["execution_validation"]["base_dir"] / RESULTS_CONFIG["execution_validation"][model_type]
            results["execution_validation"] = self._load_results_file(exec_path)
            logger.info(f"Loaded {len(results['execution_validation'])} execution validation results")
        except FileNotFoundError:
            logger.warning(f"Execution validation results not found for {model_type}")
            results["execution_validation"] = []

        # 3. åŠ è½½è¯­ä¹‰ç­‰ä»·æ€§è¯„ä¼°ç»“æœ
        try:
            semantic_path = RESULTS_CONFIG["semantic_evaluation"]["base_dir"] / RESULTS_CONFIG["semantic_evaluation"][model_type]
            results["semantic_evaluation"] = self._load_results_file(semantic_path)
            logger.info(f"Loaded {len(results['semantic_evaluation'])} semantic evaluation results")
        except FileNotFoundError:
            logger.warning(f"Semantic evaluation results not found for {model_type}")
            results["semantic_evaluation"] = []

        return results

    def _load_results_file(self, file_path: Path) -> List[Dict]:
        """
        ä»JSONLæ–‡ä»¶åŠ è½½ç»“æœ

        Args:
            file_path: ç»“æœæ–‡ä»¶è·¯å¾„

        Returns:
            ç»“æœåˆ—è¡¨
        """
        results = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        item = json.loads(line.strip())
                        if "index" not in item:
                            item["index"] = line_num - 1
                        results.append(item)
                    except json.JSONDecodeError as e:
                        logger.warning(f"Error parsing line {line_num} in {file_path}: {e}")
                        continue
        except Exception as e:
            logger.error(f"Error loading results from {file_path}: {e}")

        # æŒ‰indexæ’åº
        results.sort(key=lambda x: x.get("index", 0))
        return results

    def merge_results(self, all_results: Dict[str, List[Dict]], model_type) -> List[Dict]:
        """
        åˆå¹¶æ‰€æœ‰è¯„ä¼°æ­¥éª¤çš„ç»“æœ

        Args:
            all_results: åŒ…å«æ‰€æœ‰æ­¥éª¤ç»“æœçš„å­—å…¸

        Returns:
            åˆå¹¶åçš„ç»“æœåˆ—è¡¨
        """
        # è·å–æœ€å¤§æ ·æœ¬æ•°ï¼ˆä»¥SQLç”Ÿæˆç»“æœä¸ºåŸºå‡†ï¼‰
        max_samples = len(all_results.get("sql_generation", []))
        logger.info(f"Merging results for {max_samples} samples")

        merged_results = []

        for i in range(max_samples):
            merged_item = {
                "index": i,
                "model_type": model_type,  # å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€è®¾ç½®
            }

            # åˆå¹¶SQLç”Ÿæˆç»“æœ
            gen_results = all_results.get("sql_generation", [])
            if i < len(gen_results):
                gen_item = gen_results[i]
                merged_item.update({
                    "user_question": gen_item.get("user_question", ""),
                    "database_schema": gen_item.get("database_schema", ""),
                    "gold_sql": gen_item.get("gold_sql", ""),
                    "generated_sql": gen_item.get("generated_sql", ""),
                    "generation_success": gen_item.get("generation_success", False),
                    "raw_generation_response": gen_item.get("raw_response", ""),
                })

            # åˆå¹¶æ‰§è¡ŒéªŒè¯ç»“æœ
            exec_results = all_results.get("execution_validation", [])
            if i < len(exec_results):
                exec_item = exec_results[i]
                merged_item.update({
                    'execution_valid': exec_item.get("execution_valid", False),
                    "execution_error": exec_item.get("error_message", ""),
                })

            # åˆå¹¶è¯­ä¹‰ç­‰ä»·æ€§è¯„ä¼°ç»“æœ
            semantic_results = all_results.get("semantic_evaluation", [])
            if i < len(semantic_results):
                semantic_item = semantic_results[i]
                merged_item.update({
                    "is_semantically_equivalent": semantic_item.get("is_semantically_equivalent", False),
                    "semantic_reason": semantic_item.get("semantic_reason", ""),
                })

            # åº”ç”¨æœ€ç»ˆè¯„ä¼°è§„åˆ™
            merged_item["is_overall_correct"] = self._apply_evaluation_rule(merged_item)

            merged_results.append(merged_item)

        logger.info(f"Successfully merged {len(merged_results)} samples")
        return merged_results

    def _apply_evaluation_rule(self, merged_item: Dict) -> bool:
        """
        åº”ç”¨æœ€ç»ˆè¯„ä¼°è§„åˆ™åˆ¤æ–­æ ·æœ¬æ˜¯å¦æ­£ç¡®

        è§„åˆ™: semantic_equivalence OR query_matching

        Args:
            merged_item: åˆå¹¶åçš„è¯„ä¼°ç»“æœé¡¹

        Returns:
            æ˜¯å¦æ­£ç¡®
        """
        # è·å–å„é¡¹è¯„ä¼°ç»“æœ
        semantic_equivalent = merged_item.get("is_semantically_equivalent", False)
        query_match = merged_item.get("is_query_match", False)

        # åº”ç”¨è§„åˆ™: semantic_equivalence OR query_matching
        is_correct = semantic_equivalent or query_match

        return is_correct

    def calculate_metrics(self, merged_results: List[Dict]) -> Dict[str, Any]:
        """
        è®¡ç®—æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡

        Args:
            merged_results: åˆå¹¶åçš„ç»“æœåˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if not merged_results:
            return {"error": "No results to calculate metrics"}

        total_samples = len(merged_results)

        # ç»Ÿè®¡å„é¡¹æŒ‡æ ‡
        stats = {
            "total_samples": total_samples,
            "execution_validity": {
                "valid_count": 0,
                "invalid_count": 0,
                "validity_rate": 0
            },
            "semantic_equivalence": {
                "equivalent_count": 0,
                "nonequivalent_count": 0,
                "equivalence_rate": 0
            },
        }

        # éå†æ‰€æœ‰æ ·æœ¬è¿›è¡Œç»Ÿè®¡
        for result in merged_results:
            # æ‰§è¡Œæœ‰æ•ˆæ€§
            if result.get("execution_valid", False):
                stats["execution_validity"]["valid_count"] += 1
            else:
                stats["execution_validity"]["invalid_count"] += 1

            # è¯­ä¹‰ç­‰ä»·æ€§
            if result.get("is_semantically_equivalent", False):
                stats["semantic_equivalence"]["equivalent_count"] += 1
            else:
                stats["semantic_equivalence"]["nonequivalent_count"] += 1

        # è®¡ç®—æ¯”ç‡
        stats["execution_validity"]["validity_rate"] = (
            stats["execution_validity"]["valid_count"] / total_samples
        )
        stats["semantic_equivalence"]["equivalence_rate"] = (
            stats["semantic_equivalence"]["equivalent_count"] / total_samples
        )

        # æ·»åŠ ç”Ÿæˆæ—¶é—´æˆ³
        stats["evaluation_timestamp"] = datetime.now().isoformat()

        logger.info(f"Calculated metrics for {total_samples} samples")

        return stats

    def save_metrics(self, metrics: Dict[str, Any], save_path: Path):
        """
        ä¿å­˜è¯„ä¼°æŒ‡æ ‡åˆ°JSONæ–‡ä»¶

        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        try:
            save_path.parent.mkdir(parents=True, exist_ok=True)
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, ensure_ascii=False, indent=2)
            logger.info(f"Metrics saved to: {save_path}")
        except Exception as e:
            logger.error(f"Error saving metrics: {e}")
            raise

    def save_detailed_results(self, merged_results: List[Dict], save_path: Path):
        """
        ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶

        Args:
            merged_results: åˆå¹¶åçš„ç»“æœåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
        """
        try:
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
            df = pd.DataFrame(merged_results)
            df.to_csv(save_path, index=False, encoding='utf-8-sig')
            logger.info(f"Detailed results saved to: {save_path}")
        except Exception as e:
            logger.error(f"Error saving detailed results: {e}")
            raise

    def generate_summary_report(self, metrics: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æ‘˜è¦æŠ¥å‘Š

        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸

        Returns:
            æ‘˜è¦æŠ¥å‘Šæ–‡æœ¬
        """
        report = []
        report.append("=" * 60)
        report.append("Text-to-SQL è¯„ä¼°æ‘˜è¦æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"è¯„ä¼°æ—¶é—´: {metrics.get('evaluation_timestamp', 'Unknown')}")
        report.append(f"è¯„ä¼°è§„åˆ™: {metrics.get('evaluation_rule', {}).get('description', 'Unknown')}")
        report.append(f"æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        report.append("")

        # æ‰§è¡Œæœ‰æ•ˆæ€§
        exec_stats = metrics["execution_validity"]
        report.append("ğŸ”§ SQLæ‰§è¡Œæœ‰æ•ˆæ€§:")
        report.append(f"  - å¯æ‰§è¡Œ: {exec_stats['valid_count']} ({exec_stats['validity_rate']:.2%})")
        report.append(f"  - ä¸å¯æ‰§è¡Œ: {exec_stats['invalid_count']} ({1-exec_stats['validity_rate']:.2%})")
        report.append("")

        # è¯­ä¹‰ç­‰ä»·æ€§
        sem_stats = metrics["semantic_equivalence"]
        report.append("ğŸ¯ SQLè¯­ä¹‰ç­‰ä»·æ€§:")
        report.append(f"  - ç­‰ä»·: {sem_stats['equivalent_count']} ({sem_stats['equivalence_rate']:.2%})")
        report.append(f"  - ä¸ç­‰ä»·: {sem_stats['nonequivalent_count']} ({1-sem_stats['equivalence_rate']:.2%})")
        report.append("")

        # æ•´ä½“æ­£ç¡®æ€§
        overall_stats = metrics["overall_correctness"]
        report.append("âœ… æ•´ä½“æ­£ç¡®æ€§:")
        report.append(f"  - æ­£ç¡®: {overall_stats['correct_count']} ({overall_stats['correctness_rate']:.2%})")
        report.append(f"  - é”™è¯¯: {overall_stats['incorrect_count']} ({1-overall_stats['correctness_rate']:.2%})")
        report.append("")

        # å…³é”®æŒ‡æ ‡æ€»ç»“
        report.append("ğŸ”‘ å…³é”®æŒ‡æ ‡:")
        report.append(f"  - æœ€ç»ˆå‡†ç¡®ç‡: {overall_stats['correctness_rate']:.2%}")
        report.append(f"  - SQLå¯æ‰§è¡Œç‡: {exec_stats['validity_rate']:.2%}")
        report.append(f"  - è¯­ä¹‰ç­‰ä»·ç‡: {sem_stats['equivalence_rate']:.2%}")
        report.append("=" * 60)

        return "\n".join(report)