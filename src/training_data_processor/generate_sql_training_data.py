import json
import random
import argparse
from pathlib import Path

import re


def remove_sql_comments(sql: str) -> str:
    """
    ç§»é™¤ SQL å­—ç¬¦ä¸²ä¸­çš„æ³¨é‡Šï¼š
      - å•è¡Œæ³¨é‡Šï¼šä»¥ '-- ' å¼€å¤´ï¼ˆæ³¨æ„ç©ºæ ¼ï¼‰ï¼Œç›´åˆ°è¡Œå°¾
      - å¤šè¡Œæ³¨é‡Šï¼š/* ... */
    ä¸ä¼šè¯¯åˆ å­—ç¬¦ä¸²ä¸­çš„ '--' æˆ– '/*'ï¼ˆé€šè¿‡ç®€å•å¯å‘å¼ï¼šä»…å¤„ç†éå­—ç¬¦ä¸²ä¸Šä¸‹æ–‡ï¼Œæ­¤å¤„ä¸ºç®€åŒ–å‡è®¾æ³¨é‡Šä¸åµŒå¥—åœ¨å­—ç¬¦ä¸²ä¸­ï¼‰
    """
    if not sql:
        return sql

    # Step 1: ç§»é™¤å¤šè¡Œæ³¨é‡Š /* ... */
    # ä½¿ç”¨éè´ªå©ªåŒ¹é…ï¼Œæ”¯æŒè·¨è¡Œ
    sql_no_multiline = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)

    # Step 2: ç§»é™¤å•è¡Œæ³¨é‡Š -- ...
    # è¦æ±‚ '--' åå¿…é¡»æœ‰ç©ºæ ¼æˆ–æ¢è¡Œï¼ˆç¬¦åˆ SQL æ ‡å‡†ï¼‰
    # æ³¨æ„ï¼šä¸èƒ½ç®€å•æ›¿æ¢æ‰€æœ‰ '--'ï¼Œå› ä¸ºå¯èƒ½å‡ºç°åœ¨æ ‡è¯†ç¬¦æˆ–å­—ç¬¦ä¸²ä¸­ï¼ˆå¦‚ 'abc--def'ï¼‰
    # æ­¤å¤„é‡‡ç”¨é€è¡Œå¤„ç†æ›´å®‰å…¨
    lines = sql_no_multiline.split('\n')
    cleaned_lines = []
    for line in lines:
        # æŸ¥æ‰¾ '-- 'ï¼ˆæ³¨æ„ç©ºæ ¼ï¼‰çš„ä½ç½®
        idx = line.find('--')
        if idx != -1:
            line = line[:idx]
        # å¯é€‰ï¼šè¿›ä¸€æ­¥å»é™¤è¡Œå°¾ç©ºæ ¼
        cleaned_lines.append(line.rstrip())

    result = '\n'.join(cleaned_lines).strip()
    # å¦‚æœç»“æœä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²è€Œé None
    return result if result else ''


def load_annotated_ddl(input_path: str):
    """ä» JSONL æ–‡ä»¶ä¸­åŠ è½½å¸¦æ³¨é‡Šçš„ DDL æ•°æ®è®°å½•ã€‚"""
    annotated_records = []
    with open(input_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                annotated_records.append(json.loads(line))
    return annotated_records


def load_prompt_template(template_path: str) -> str:
    """ä»æ–‡æœ¬æ–‡ä»¶ä¸­è¯»å–æç¤ºæ¨¡æ¿ã€‚"""
    with open(template_path, encoding="utf-8") as f:
        return f.read().strip()


def build_conversations(annotated_records, prompt_template: str):
    """
    æ ¹æ®æ¯æ¡è®°å½•å’Œæç¤ºæ¨¡æ¿ï¼Œæ„é€ åŒ…å«ç”¨æˆ·æç¤ºå’ŒçœŸå® SQL çš„å¯¹è¯æ•°æ®ã€‚
    åŒæ—¶è‡ªåŠ¨æ¸…ç† ground_truth SQL ä¸­çš„æ³¨é‡Šã€‚

    è¿”å›æ ¼å¼ä¸ºåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«ï¼š
      - prompt: å¯¹è¯åˆ—è¡¨ï¼ˆå½“å‰ä»…ç”¨æˆ·æ¶ˆæ¯ï¼‰
      - ground_truth: å¯¹åº”çš„çœŸå® SQL è¯­å¥
    """
    conversations = []
    for record in annotated_records:
        # æå–å…ƒæ•°æ®
        metadata = record["question_synthesis_metadata"]
        prompt_context = record["prompt_context"]["metadata"]

        engine = metadata["engine"]
        schema_desc = metadata["schema"]
        sample_data = prompt_context["db_value_prompt"]
        user_question = record["synthesis_question"]
        ground_truth_sql = metadata["sql"]

        # ğŸ”¥ æ–°å¢ï¼šæ¸…ç† SQL ä¸­çš„æ³¨é‡Š
        clean_sql = remove_sql_comments(ground_truth_sql)

        # å¡«å……æ¨¡æ¿
        formatted_prompt = (
            prompt_template
            .replace("{database_engine}", engine)
            .replace("{schema_description}", schema_desc)
            .replace("{sample_data}", json.dumps(sample_data, ensure_ascii=False, indent=2))
            .replace("{user_question}", user_question)
        )

        # æ„é€ å•æ¡è®­ç»ƒæ ·æœ¬
        conversation = {
            "prompt": [{"role": "user", "content": formatted_prompt}],
            "ground_truth": clean_sql  # ä½¿ç”¨æ¸…ç†åçš„ SQL
        }
        conversations.append(conversation)
    return conversations


def split_train_test(data, test_ratio=0.2, seed=42):
    """å°†æ•°æ®æŒ‰æŒ‡å®šæ¯”ä¾‹éšæœºåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚"""
    random.seed(seed)
    random.shuffle(data)
    split_index = int(len(data) * (1 - test_ratio))
    return data[:split_index], data[split_index:]


def save_jsonl(data, output_path: str):
    """å°†æ•°æ®åˆ—è¡¨ä¿å­˜ä¸º JSONL æ ¼å¼æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰ã€‚"""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
    with open(output_path, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
            f.flush()


def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‚æ•°ã€åŠ è½½æ•°æ®ã€ç”Ÿæˆæ ·æœ¬ã€åˆ’åˆ†å¹¶ä¿å­˜ç»“æœã€‚"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆç”¨äº SQL ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒä¸æµ‹è¯•æ•°æ®é›†")
    parser.add_argument(
        "--input-jsonl",
        type=str,
        default="../data_synthesis/sql_query_match_validation/output/final/static_requirement_matching/annotated_ddl.jsonl",
        help="è¾“å…¥çš„å¸¦æ³¨é‡Š DDL æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰"
    )
    parser.add_argument(
        "--prompt-template",
        type=str,
        default="prompts/sql_generation_prompt.txt",
        help="æç¤ºæ¨¡æ¿æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="../../data",
        help="è¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜ train.jsonl å’Œ test.jsonl"
    )
    parser.add_argument(
        "--test-ratio",
        type=float,
        default=0.2,
        help="æµ‹è¯•é›†å æ¯”ï¼Œé»˜è®¤ä¸º 0.2ï¼ˆå³ 8:2 åˆ’åˆ†ï¼‰"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼Œç¡®ä¿æ•°æ®åˆ’åˆ†å¯å¤ç°"
    )
    args = parser.parse_args()

    # åŠ è½½åŸå§‹æ•°æ®å’Œæç¤ºæ¨¡æ¿
    annotated_ddl = load_annotated_ddl(args.input_jsonl)
    prompt_template = load_prompt_template(args.prompt_template)

    # æ„å»ºå¯¹è¯æ ¼å¼è®­ç»ƒæ ·æœ¬
    conversations = build_conversations(annotated_ddl, prompt_template)
    print(f"å…±ç”Ÿæˆæ ·æœ¬æ•°é‡: {len(conversations)}")

    # åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    train_data, test_data = split_train_test(
        conversations,
        test_ratio=args.test_ratio,
        seed=args.seed
    )
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}ï¼Œæµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir)
    save_jsonl(train_data, output_dir / "train.jsonl")
    save_jsonl(test_data, output_dir / "test.jsonl")

    print(f"è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å·²ä¿å­˜è‡³ç›®å½•: {output_dir.resolve()}")


if __name__ == "__main__":
    main()